\section{Diskussion}
% Zusammenfassung der Ergebnisse hinsichtlich der Evaluationsziele; 
Es lässt sich zusammenfassend beschreiben, dass bei jeder der erhobenen quantitativen Variablen eine deskriptive Überlegenheit Trellos besteht, keiner dieser Unterschiede jedoch signifikant wurde. 
Auch auf qualitativer Seite ähnelt sich sowohl das Lob als auch die Kritik an den beiden Systemen sehr stark, die häufigsten Kategorien waren für beide Systeme (bis auf die Verteilung) identisch.
Damit bewahrheiten sich unsere Hypothesen nicht, und es konnte keine Überlegenheit Trellos aus Usabilityperspektive festgestellt werden. 



\subsection{Erklärungsansätze}

Mehrere verschiedene Ursachen für dieses Ergebnis sind plausibel. 
Setzt man die Validität, Reliabilität und Objektivität der Studie voraus, so verbleiben zwei mögliche Interpretationen; 
\begin{seriate}
\item Es ist kein Unterschied zwischen den beiden Systemen vorhanden und der deskriptive Unterschied ist zufallsbedingt, oder
\item Es ist ein Effekt vorhanden, dieser ist jedoch so gering, dass er durch den gegebenen Versuchsaufbau nicht entdeckt werden konnte.
\end{seriate}

\subsubsection{Poweranalyse}
Um die Wahrscheinlichkeit dieser Erklärungen zu untersuchen führten wir eine Post-Hoc Poweranalyse durch. Die dabei ermittelten erzielten Teststärken sind im Vergleich zum Standard von $1-\beta~=~.80$ in jedem Fall zu gering ausgefallen (jeweils $1-\beta<.490$). Wie an der Abbildung \ref{fig:powertlx} zum NASA TLX (Test mit der größten Teststärke) zu sehen, hätte die Stichprobe mit mindestens 65 Teilnehmern deutlich größer ausfallen müssen um die Standard-Teststärke zu erreichen, in allen weiteren Tests noch deutlich mehr. Auch waren wir in zwei von sechs Fällen gezwungen, den testschwächeren Test zu rechnen, der ohne die Normalverteilungsannahme auskommt.
Dies beweist nicht die Existenz eines Effekts, zeigt aber, dass es mit dem gegebenen Versuchsdesign nicht möglich gewesen wäre, einen so kleinen Effekt nachzuweisen, selbst wenn dieser vorhanden wäre.

\subsubsection{Ähnlichkeit}
Ein Indiz, welches dafür spricht, dass in Wirklichkeit kein Unterschied bezüglich der Usability der beiden Systeme besteht, stellt die extrem ähnliche Funktionalität dar, die starke Orientierung Zenkits an Trello, und, wie im qualitativen Ergebnisteil berichtet, die Ähnlichkeit der auftretenden Probleme und des Lobs. 








\subsection{Mögliche Konfundierungen}

\subsubsection{Zeitmessung}
Ein möglicherweise konfundierender Faktor ist die Berechnung der durchschnittlichen Zeit pro Aufgabe: Es mussten pro Aufgabe mindestens zwei von drei Unteraufgaben erfüllt werden, sowie die Maximalzeit unterschritten werden, ansonsten wurde letztere als benötigte Zeit für die Aufgabe angesetzt. Hier könnte es zu einem Deckeneffekt und einem Problem der Konstruktvalidität gekommen sein, da es fraglich ist, ob diese getroffenen Festlegungen zur Operationalisierung der Effizienz gültig sind.

\subsubsection{Valenzmethode}
Auch bei der Nutzung der Marker als Berechnungsgrundlage als Maß der Zufriedenheit ist eine Konfundierung möglich. Beispielsweise erfuhren wir im Interview, dass es Fälle gab in denen eine Erkenntnis bei beiden Systemen entstand, aber nur bei einem der beiden zum Setzen eines Markers führte, da die Versuchsperson Marker nicht redundant verwenden wollte. Die verfügbare Zeit zur Exploration und zum Setzen der Marker war mit zwei Minuten möglicherweise zu knapp bemessen, sodass einige Versuchspersonen rein aufgrund dessen nur sehr wenige (oder auch gar keine) Marker setzten. 

\subsubsection{Versuchssituation}
Weiterhin könnte die künstliche Versuchssituation die Ergebnisse beeinflusst haben. Besonders die intensive Beobachtung durch den Protokollanten, der hinter der Versuchsperson sitzend die Erfüllung der Teilaufgaben notierte, könnte die Versuchsperson nervös gemacht haben.

\subsubsection{Aufgabengestaltung}
Bei der Gestaltung der Aufgaben (siehe Anhang) kann insbesondere die Repräsentativität in Frage gestellt werden. Beispielsweise ist das zur Exploration genutzte Board 'Einkaufsliste' ein wenig wahrscheinlicher Anwendungsfall der Projektmanagementtools. Auch die abgefragten Funktionalitäten, allen voraus das Auffinden des Archivs und der anschließenden Wiederherstellung einer Karte aus demselben, sind möglicherweise kein passendes Abbild der Funktionen, die man im alltäglichen Nutzungskontext benötigen würde.

Dieser Aspekt ist auch aufgrund der entstehenden Folgen kritisch: Sollte es sich zum Beispiel beim Auffinden des Archivs um einen sehr selten auftretenden Use Case handeln, wäre dies mehr ein konstruiertes als ein reales Problem, und damit auch dessen Bedeutung in unserer Studie unverhältnismäßig groß.

Ein weiterer, eng verwandter Kritikpunkt des Studiendesigns ist die eingeschränkte Unabhängigkeit der Aufgaben. Obwohl aktiv versucht wurde diese voneinander unabhängig zu gestalten, fiel bei der Auswertung auf, dass die beiden Teilaufgaben mit der geringsten Erfolgsquote nicht die von uns als schwerste empfundene Teilaufgabe (Nr. 13) war, sondern eine der nachfolgenden, für die nach der 'schweren' Aufgabe dann keine zeitlichen und mentalen Ressourcen mehr verfügbar waren (Nr. 14 bei Trello mit einer Erfolgsquote von 53.57\%, Nr. 15 bei Zenkit mit einer Erfolgsquote von 42.86\%). 


\subsection{Überlegungen zu Folgestudien}
Aufgrund der geringen erreichten Teststärke und dem deskriptiven aber nicht signifikanten Unterschied wäre eine Folgestudie sinnvoll, um die Frage nach einem Effekt zwischen Trello und Zenkit aus Usability-Perspektive weiter zu untersuchen.

Ein großer Mangel der Studie ist die Vernachlässigung des Alleinstellungsmerkmals von Zenkit, den zusätzlichen Ansichten. Um eine stärkere Vergleichbarkeit zu schaffen und uns auf die Grundfunktionen zu beschränken, haben wir uns dazu entschieden, diese außen vor zu lassen. Gleichzeitig ging damit der Verlust eines der wichtigsten Argumente einher, Zenkit über Trello zu präferieren, sodass man diesen Aspekt in einer Folgestudie berücksichtigen sollte.
    
Die Explorationsphase der Studie hätte davon profitiert, die Instruktion noch präziser und standardisierter zu formulieren, und eventuell mehr Zeit zur Verfügung zu stellen. 
Die Beobachtung durch den Protokollanten im Anschluss sollte bei einer Folgestudie eventuell durch eine spätere digitale Auswertung ausgetauscht werden, um den Effekt der Versuchssituation abzuschwächen.

Außerdem sollten die Aufgaben unabhängiger und repräsentativer gestaltet und durch User Research, zum Beispiel durch Feldbeobachtung, ergänzt und validiert werden. Auch die Berechnung der durchschnittlichen Zeit pro Aufgabe (als Maß der Effizienz) sollte aufgrund der Validitäts- und Deckeneffektsbedenken überarbeitet werden. Hier könnte zusätzlich durch eine explorative Datenanalyse überprüft werden, ob nicht möglicherweise bestimmte Aufgaben bei einem System signifikant leichter oder schwerer waren als bei dem jeweils anderen.

Die Stichprobengröße sollte erhöht werden, um die Teststärke mindestens auf das Standardniveau zu heben. Die Ermittlung der Anzahl Probanden sollte ausgehend von der Poweranalyse und dem geringen Effekt, der in dieser Studie deskriptiv zu erkennen war, erfolgen.




\subsection{Eingesetzte Methoden}
Beide in dieser Studie eingesetzten Methoden, die Heuristische Evaluation und der Usability-Test, eignen sich besonders zur Anwendung in der frühen Phase des Design Prozesses eines Systems. Die Entdeckung und Auflösung von Usability Problemen vor dem Aufwand vieler Stunden Entwicklungszeit kann effektiv Zeit und Kosten sparen, und gleichzeitig die User Experience maßgeblich verbessern. 

Ein Schwachpunkt der Heuristischen Evaluation ist das Finden von Problemen, die mehr potenzieller als realer Natur sind; Schließlich ist in den meisten Fällen der Evaluator nicht der Zielnutzer, sodass hier eine Konflikt entstehen kann. Auch hängen die Ergebnisse von der Wahl der Evaluatoren ab, die besonders in unserem Fall aufgrund der homogenen und eher wenig erfahrenen Zusammensetzung zu kritisieren ist.
Andererseits konnten dank dem Einsatz der Heuristiken auch Probleme entdeckt werden, die dem User im normalen Gebrauch tendenziell weniger auffallen, wie z.B. Inkonsistenzen im Design oder der Informationsstruktur.

Beim Usability-Test fiel auf, dass realitätsnähere und durch simulierte User validierte Probleme gefunden wurden. Das wird jedoch gleichzeitig relativiert durch die Abhängigkeit von den Aufgaben jeden Usability-Tests. Hier könnte ein Bias entstanden sein, da wir die Aufgaben auf Basis der Heuristischen Evaluation gestaltet haben, sodass diese möglicherweise Probleme forcierten, welche die Versuchspersonen unter 'aufgabenfremden' Umständen nicht entdeckt hätten.

Rückblickend lässt sich zusammenfassen, dass die eingesetzten Methoden mit den oben beschriebenen Einschränkungen sehr gut zur Erfüllung unserer Evaluationsziele geeignet waren.


\subsection{Fazit und Ausblick}
Die Studie konnte keine finale Antwort darauf geben welches der beiden Systeme aus Sicht der Usability überlegen ist, daher sollte eine weitere Untersuchung folgen, bei der die angesprochenen Überlegungen umgesetzt werden.

Da sich die beiden Systeme in der quantitativen Auswertung des Usability-Tests nur wenig unterschieden, haben wir uns bei der Problemauswahl für die Verbesserungsvorschläge vor allem an der Heuristischen Evaluation orientiert. Um den Effekt der Verwirrung des Nutzers einzuschränken wurde bei Trello für die Problemgrade 1-4 jeweils ein Verbesserungsvorschlag graphisch aufbereitet (siehe Abbildungen \ref{fig:datepicker}, \ref{fig:archiv}, \ref{fig:feedback}, \ref{fig:vorlage} im Anhang). Das Verbesserungspotential durch diese Änderungen wird v.a. durch deren eher gering eingeschätzte Nutzungsfrequenz limitiert, bietet aber unserer Einschätzung nach jeweils eine erwartungskonformere Variante und führt so zu weniger Frustration und Verwirrung.

Von den bei Zenkit überarbeiteten Aspekten erwarten wir eine höhere Nutzungsfrequenz, was deren Verbesserungspotential erhöht. Hier wurde das Hauptaugenmerk auf die Lösung des Problems des schwierigen Findens von Bedienelementen gelegt.